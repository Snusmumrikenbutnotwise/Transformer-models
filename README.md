# Transformer-models
Implementation of transformer architecture based on GPT-OSS. 
It is a continuation of coding project done for Deep Neural Networks course on University of Warsaw. 
Architecture modules implemented:
- SwiGLU module
- Mixture of Experts
- Grouped Query Attention
- Sliding Window Attention
- Rotatatory Positional Embedding

Examples of small GPT-like models were also trained on three datasets of various difficulties.
Right now there is an issue with github rendering of the notebook. It will work in vsc, jupyter or colab :)
Here is a link to colab version: https://colab.research.google.com/drive/1ju12WMmkNMMjpsXikdsjZGOssdDi9mBX?usp=sharing
