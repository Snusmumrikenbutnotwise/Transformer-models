# Transformer-models
Implementation of transformer architecture based on GPT-OSS. 
It is a continuation of coding project done for Deep Neural Networks course on University of Warsaw. 
Architecture modules implemented:
- SwiGLU module
- Mixture of Experts
- Grouped Query Attention
- Sliding Window Attention
- Rotatatory Positional Embedding

Examples of small GPT-like models were also trained on three datasets of various difficulties.
